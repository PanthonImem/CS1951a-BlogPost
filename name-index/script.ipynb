{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"script.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"j180BsxQmWVm","colab_type":"code","colab":{}},"cell_type":"code","source":["import threading\n","import functools\n","import string\n","\n","import requests\n","import json\n","import bs4"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mdfpDW2JmWVp","colab_type":"code","colab":{}},"cell_type":"code","source":["def threadify(func):\n","    \"function runs in a new thread.\"\n","\n","    @functools.wraps(func)\n","    def run(*args, **kwds):\n","        new_thread = threading.Thread(\n","            target = func,\n","            args   = args,\n","            kwargs = kwds)\n","        new_thread.start()\n","        return new_thread\n","\n","    return run"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zsfTNd-rmWVr","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_names(url):\n","    resp = requests.get(url)\n","    if resp.status_code != 200:\n","        print(\"Error:\", resp.text)\n","        return [], url # try again!\n","    soup = bs4.BeautifulSoup(resp.text, 'html.parser')\n","    page = soup.find_all(class_=\"mw-category-group\")\n","    data = [(i.text, i[\"href\"]) for e in page for i in e.find_all(\"a\")]\n","    next_url = soup.find(\"a\", text=\"next page\")\n","    return data, next_url['href'] if next_url else None"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dcTgwaW_mWVt","colab_type":"code","colab":{}},"cell_type":"code","source":["@threadify\n","def collect(url, collector):\n","    out = []\n","    try:\n","        while url:\n","            out, url = get_names('https://en.wikipedia.org' + url)\n","            if out[0] in collector:\n","                break\n","            collector.update(out)\n","        print(\"breakpoint reached!\")\n","    except:\n","        # if there's an error just collect(url, collector)\n","        # again once there's space for another thread\n","        print(\"error when collecting:\", url)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pCDoDcXrmWVv","colab_type":"code","colab":{}},"cell_type":"code","source":["LIVING_PEOPLE_MAIN_PAGE = \"/wiki/Category:Living_people\"\n","results = set()\n","pages = [LIVING_PEOPLE_MAIN_PAGE]\n","for c in string.ascii_uppercase:\n","    pages.append(f\"/wiki/Category:Living_people?from={c}\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"puq1p0IJmWVw","colab_type":"code","colab":{}},"cell_type":"code","source":["threads = []\n","for p in pages:    \n","    threads.append(collect(p, results))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7GDoUTRbmWVy","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","for i in range(500):\n","    print(\"\\r\", i, len(results), end=\" \")\n","    time.sleep(0.5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BZ1PqrIVmWVz","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(\"names.json\", \"w\") as f:\n","    json.dump(sorted(results), f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m61n0XIJmWV1","colab_type":"code","colab":{}},"cell_type":"code","source":["import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xhAkDBj6dWZ0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"TJHb2LLYdYyv","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}